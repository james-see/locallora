# Axolotl config for LoRA fine-tuning on Apple Silicon M4 Max
# Based on Setup-Notes-Config.md

base_model: mistralai/Ministral-8B-Instruct-2410
tokenizer_type: AutoTokenizer
tokenizer_use_fast: true

# Disable quantization (not supported on MPS)
load_in_8bit: false
load_in_4bit: false

# Dataset configuration
datasets:
  - path: ./papers_text.jsonl
    type: completion
    field: text

dataset_prepared_path: ./prepared_data
val_set_size: 0
output_dir: ./output/lora-m4max

# LoRA adapter configuration
adapter: lora
lora_r: 8
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj

# Tokenizer and sequence settings
sequence_len: 2048
sample_packing: false
pad_to_sequence_len: false

# Training hyperparameters
gradient_accumulation_steps: 16
micro_batch_size: 1
num_epochs: 3
learning_rate: 2.0e-5
weight_decay: 0.0
warmup_steps: 100
lr_scheduler: cosine
optimizer: adamw_torch
max_grad_norm: 1.0

# Precision settings for Apple Silicon
# bf16 generally works on M-series chips, toggle if issues arise
bf16: true
fp16: false
tf32: false

# Memory optimization
gradient_checkpointing: true

# Logging and checkpoints
logging_steps: 50
save_strategy: steps
save_steps: 500
save_total_limit: 5
eval_steps: 500

# Hardware configuration for MPS
device_map: auto

# Disable distributed training (single device)
deepspeed: 
ddp: false
